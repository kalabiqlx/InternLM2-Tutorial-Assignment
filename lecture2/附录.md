# pip 换源及 conda 换源方法
对于 pip 换源，需要临时使用镜像源安装，如下所示：' some-package '为你需要安装的包名

	pip install -i https://mirrors.cernet.edu.cn/pypi/web/simple some-package

设置' pip '默认镜像源，升级' pip '到最新的版本 (>=10.0.0) 后进行配置，如下所示：

	python -m pip install --upgrade pip
	pip config set global.index-url   https://mirrors.cernet.edu.cn/pypi/web/simple
  
如果您的 pip 默认源的网络连接较差，可以临时使用镜像源升级 pip：

	python -m pip install -i https://mirrors.cernet.edu.cn/pypi/web/simple --upgrade pip
 
对于 conda 换源，镜像站提供了 Anaconda 仓库与第三方源（conda-forge、msys2、pytorch 等），各系统都可以通过修改用户目录下的 .condarc 文件来使用镜像站。不同系统下的 .condarc 目录如下：

* Linux: ${HOME}/.condarc
* macOS: ${HOME}/.condarc
* Windows: C:\Users\<YourUserName>\.condarc
  
注意：

* Windows 用户无法直接创建名为 .condarc 的文件，可先执行 conda config --set show_channel_urls yes 生成该文件之后再修改。
  
快速配置

	cat <<'EOF' > ~/.condarc
	channels:
	- defaults
	show_channel_urls: true
	default_channels:
	- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
	- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
	- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
	custom_channels:
	conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
	pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
	EOF

# 模型下载

## Hugging Face 
使用'Hugging Face'官方提供的'huggingface-cli'命令行工具。安装依赖:

	pip install -U huggingface_hub
 
然后新建 'python' 文件，填入以下代码，运行即可。

* resume-download：断点续下
* local-dir：本地存储路径。
  
其中 linux 环境下需要填写绝对路径.

	import os
	# 下载模型
	os.system('huggingface-cli download --resume-download internlm/internlm2-chat-7b --local-dir your_path')
 
以下内容将展示使用 'huggingface_hub' 下载模型中的部分文件

	import os 
	from huggingface_hub import hf_hub_download  # Load model directly 
	
	hf_hub_download(repo_id="internlm/internlm2-7b", filename="config.json")
 
## ModelScope 
使用 'modelscope' 中的 'snapshot_download' 函数下载模型，第一个参数为模型名称，参数 'cache_dir' 为模型的下载路径。

注意：'cache_dir' 最好为绝对路径。

安装依赖：

	pip install modelscope==1.9.5
	pip install transformers==4.35.2
 
在当前目录下新建 python 文件，填入以下代码，运行即可。

	import torch
	from modelscope import snapshot_download, AutoModel, AutoTokenizer
	import os
	model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b', cache_dir='your path', revision='master')
 
## OpenXLab 
'OpenXLab' 可以通过指定模型仓库的地址，以及需要下载的文件的名称，文件所需下载的位置等，直接下载模型权重文件，使用 'download' 函数导入模型中心的模型。

	import torch
	import os
	from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
	base_path = './local_files'
	os.system('apt install git')
	os.system('apt install git-lfs')
	os.system(f'git clone https://code.openxlab.org.cn/Usr_name/repo_name.git {base_path}')
	os.system(f'cd {base_path} && git lfs pull')
 
## 软链接清除方法
当我们建立安全链接之后，如果想要将其删除可以选择以下命令：

	unlink link_name
 
我们举一个例子，当我想删除软链接 '/root/demo/internlm2-chat-7b' 时：

	cd /root/demo/
	unlink internlm2-chat-7b
